{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises"
      ],
      "metadata": {
        "id": "1itx-oheYjTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the [MIT-BIH Arrhythmia](https://physionet.org/content/mitdb/1.0.0/) dataset we used in the previous notebook. Now, if you want, you can implement different types of neural networks to solve the classification problem at hand."
      ],
      "metadata": {
        "id": "bOaNLBbpYqwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "LiaZCV5oa6mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from google.colab import drive\n",
        "from scipy.signal import resample\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "BVt-2znqa7iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilities"
      ],
      "metadata": {
        "id": "624ZRfbSZelF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, some utility functions are defined. In particular, `train` and `test` are the functions used to train and test models, respectively."
      ],
      "metadata": {
        "id": "W0fII8iVZk4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function.\n",
        "def train(epoch, model, loader, criterion, optimizer, device='cpu'):\n",
        "    l = 0\n",
        "    for data in tqdm(loader, desc=f'Epoch {epoch+1:03d}'):\n",
        "        x = data[0].to(device)\n",
        "        y = data[1].to(device)\n",
        "        out = model(x)\n",
        "        loss = criterion(out, y)\n",
        "        l += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return l\n",
        "\n",
        "# Test function.\n",
        "def test(model, loader, criterion, device='cpu'):\n",
        "    l = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            x = data[0].to(device)\n",
        "            y = data[1].to(device)\n",
        "            out = model(x)\n",
        "            l += criterion(out, y)\n",
        "            _, pred = torch.max(out.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (pred == y).sum().item()\n",
        "            y_true += y.tolist()\n",
        "            y_pred += pred.tolist()\n",
        "    return l, correct / total, f1_score(y_true, y_pred, average='macro'), y_true, y_pred"
      ],
      "metadata": {
        "id": "xAlJnJskZmB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cell defines three classes (`Stretch`, `Amplify`, and `Augment`) for randomly augmenting input signals."
      ],
      "metadata": {
        "id": "TZmA1PqSZpOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly stretches the signal.\n",
        "class Stretch:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        n = x.shape[0]\n",
        "        l = int(n * (1 + (random.random() - 0.5) / 3))\n",
        "        y = resample(x, l)\n",
        "        if l < n:\n",
        "            y_ = np.zeros(shape=(n,))\n",
        "            y_[:l] = y\n",
        "        else:\n",
        "            y_ = y[:n]\n",
        "        return y_\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Stretch'\n",
        "\n",
        "# Randomly amplifies the signal.\n",
        "class Amplify:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, x):\n",
        "        alpha = (random.random() - 0.5)\n",
        "        factor = -alpha * x + (1 + alpha)\n",
        "        return x * factor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'Amplify'\n",
        "\n",
        "# Randomly augments the input signal.\n",
        "class Augment:\n",
        "    def __init__(self, aug_list, verbose=False):\n",
        "        self.aug_list = aug_list\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __call__(self, x):\n",
        "        augs = ''\n",
        "        for i, aug in enumerate(self.aug_list):\n",
        "            if np.random.binomial(1, 0.5) == 1:\n",
        "                x = aug(x)\n",
        "                augs += f'{aug}, ' if i < len(self.aug_list) - 1 else f'{aug}'\n",
        "        if not self.verbose:\n",
        "            return x\n",
        "        return x, augs"
      ],
      "metadata": {
        "id": "pePKS_n3Zfte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `CustomDataset` class encapsulates the functionality required to prepare custom data for training deep learning models in PyTorch."
      ],
      "metadata": {
        "id": "HT4KUxt5ZsF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset, as we discussed in the first lecture (PyTorch Basics).\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    # Stores the data.\n",
        "    def __init__(self, x, y=None, transforms=None):\n",
        "        super().__init__()\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Returns the length of the dataset.\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    # Returns a (x, y) pair from the dataset.\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x.iloc[idx, :]\n",
        "        if self.transforms is not None:\n",
        "            x = self.transforms(x)\n",
        "        x = torch.tensor(x).float().unsqueeze(-1)\n",
        "        y = torch.tensor([self.y.iloc[idx]]).type(torch.LongTensor).squeeze()\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "iFxG5npFZtch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "AH3pNzGtZF69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run the following cells, you should first download the dataset and then upload the two files, `mitbih_train.csv` and `mitbih_test.csv`, here on Google Colab. Otherwise, you can upload the files on Google Drive and access them by connecting Colab to Drive."
      ],
      "metadata": {
        "id": "k9QuU8pXZMYX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1FRf8q2YTJg"
      },
      "outputs": [],
      "source": [
        "# Connect Google Drive.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Read train and test data.\n",
        "df_train = pd.read_csv('drive/MyDrive/mitbih_train.csv', header=None)\n",
        "df_test = pd.read_csv('drive/MyDrive/mitbih_test.csv', header=None)\n",
        "\n",
        "# Training dataframe.\n",
        "df_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the batch size and then create the `Dataset` objects."
      ],
      "metadata": {
        "id": "tqTbnu3IZTZz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size.\n",
        "batch_size = 64\n",
        "\n",
        "# New Augment object.\n",
        "augment = Augment([Amplify(), Stretch()])\n",
        "\n",
        "# Dataset objects.\n",
        "train_dataset = ...\n",
        "test_dataset = ..."
      ],
      "metadata": {
        "id": "PzLno4h8ZU9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then create the `WeightedRandomSampler` object to perform data augmentation, and define the `DataLoader` objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "jURyEUWc_9vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of training labels.\n",
        "train_targets = ...\n",
        "\n",
        "# Computing class weights based on class frequency.\n",
        "cls_weights = ...\n",
        "\n",
        "# Resulting array of weights.\n",
        "weights = ...\n",
        "\n",
        "# Weighted random sampler. Used to consider copies of minority classes.\n",
        "sampler = ...\n",
        "\n",
        "# DataLoader objects.\n",
        "train_loader = ...\n",
        "test_loader = ..."
      ],
      "metadata": {
        "id": "7PHScde2AMUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Definition"
      ],
      "metadata": {
        "id": "uOWUCrzqaPXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we select the device."
      ],
      "metadata": {
        "id": "hetarXX_bEAH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "RVXD8-PvaQ_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define the model."
      ],
      "metadata": {
        "id": "DbB9LwBbeQ3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_1$: Feature Extraction $+$ MLP"
      ],
      "metadata": {
        "id": "Rojslo1lbITV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define an MLP that takes as input a tensor of statistical features extracted from each input batch and returns the classification scores.\n",
        "\n",
        "You can consider different types of statistical features: [mean](https://pytorch.org/docs/stable/generated/torch.mean.html), [standard deviation](https://pytorch.org/docs/stable/generated/torch.std.html), [max](https://pytorch.org/docs/stable/generated/torch.max.html), [min](https://pytorch.org/docs/stable/generated/torch.min.html), etc."
      ],
      "metadata": {
        "id": "utqN0b-eBfHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters.\n",
        "...\n",
        "\n",
        "# Refer to the 0-pytorch-basics.ipynb notebook.\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, ...):\n",
        "        super(MLP, self).__init__()\n",
        "        # Define multiple nn.Linear layers.\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Extract statistical features from the input.\n",
        "        # 2. Forward pass through nn.Linear layers.\n",
        "        ...\n",
        "\n",
        "# New model.\n",
        "model = MLP(...).to(device)"
      ],
      "metadata": {
        "id": "TjWVObd2bhCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_2$: 1D-Convolutional Backbone $+$ Classification Head"
      ],
      "metadata": {
        "id": "YYz0AwFzbiCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a 1D-CNN composed of a 1D-convolutional backbone that extracts features from the input signal, and a final classification head used to produce the classification scores.\n",
        "\n",
        "Pay attention to the input and output shape of each layer, as discussed earlier in the course."
      ],
      "metadata": {
        "id": "vpL_WMmJCaAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters.\n",
        "...\n",
        "\n",
        "# Refer to the 1-cnns.ipynb notebook.\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, ...):\n",
        "        super(CNN, self).__init__()\n",
        "        # Define multiple nn.Conv1d layers.\n",
        "        # Define a single nn.Linear which acts as the final classification head.\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Forward pass through nn.Conv1d layers.\n",
        "        # 2. Forward pass through the nn.Linear layer.\n",
        "        ...\n",
        "\n",
        "# New model.\n",
        "model = CNN(...).to(device)"
      ],
      "metadata": {
        "id": "nRDlawtpbtB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_3$: 1D-Convolutional Feature Extractor $+$ LSTM Layer $+$ Classification Head"
      ],
      "metadata": {
        "id": "oh0fHOHibtVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a model composed of one or more 1D-convolutional layers, an LSTM layer, and a final classification head used to produce the classification scores.\n",
        "\n",
        "Pay attention to the input and output shape of each layer, as discussed earlier in the course."
      ],
      "metadata": {
        "id": "HV_PMYg1C2TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters.\n",
        "...\n",
        "\n",
        "# Refer to the 1-cnns.ipynb, 2-rnns.ipynb notebooks.\n",
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, ...):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        # Define one or more nn.Conv1d layers to extract features.\n",
        "        # Define a single LSTM layer.\n",
        "        # Define a single nn.Linear which acts as the final classification head.\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Forward pass through nn.Conv1d layers.\n",
        "        # 2. Forward pass through the nn.LSTM layer.\n",
        "        # 3. Forward pass through the nn.Linear layer.\n",
        "        ...\n",
        "\n",
        "# New model.\n",
        "model = CNN_LSTM(...).to(device)"
      ],
      "metadata": {
        "id": "SdLsTR6Sb4vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Test"
      ],
      "metadata": {
        "id": "OD541QwhaTpe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define few other parameters:\n",
        "- The duration of training (`num_epochs`).\n",
        "- The pace at which our model learns (`learning_rate`)."
      ],
      "metadata": {
        "id": "YrhIbIjYaZmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training hyperparameters.\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001"
      ],
      "metadata": {
        "id": "n2gCR38iaXOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we train and test the model."
      ],
      "metadata": {
        "id": "fg2B1hMxalh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_1$: Feature Extraction $+$ MLP"
      ],
      "metadata": {
        "id": "HZcn90jpfT8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training and test.\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(epoch, model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc, test_f1, y_true, y_pred = test(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch+1:03d}: training loss {train_loss:.4f}, test loss {test_loss:.4f}, test acc {test_acc:.4f}, test f1 {test_f1:.4f}')"
      ],
      "metadata": {
        "id": "tDZm37XkaeCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "x1n1VqLdaq3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_2$: 1D-Convolutional Backbone $+$ Classification Head"
      ],
      "metadata": {
        "id": "fi8t4tX5feig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training and test.\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(epoch, model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc, test_f1, y_true, y_pred = test(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch+1:03d}: training loss {train_loss:.4f}, test loss {test_loss:.4f}, test acc {test_acc:.4f}, test f1 {test_f1:.4f}')"
      ],
      "metadata": {
        "id": "QMDF2c8Xfeiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "sz2VljLOfeiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $m_3$: 1D-Convolutional Feature Extractor $+$ LSTM Layer $+$ Classification Head"
      ],
      "metadata": {
        "id": "_t_34to7ffy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training and test.\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(epoch, model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc, test_f1, y_true, y_pred = test(model, test_loader, criterion, device)\n",
        "    print(f'Epoch {epoch+1:03d}: training loss {train_loss:.4f}, test loss {test_loss:.4f}, test acc {test_acc:.4f}, test f1 {test_f1:.4f}')"
      ],
      "metadata": {
        "id": "CtG4unCVffy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred, zero_division=0))"
      ],
      "metadata": {
        "id": "5YYbQegTffy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}